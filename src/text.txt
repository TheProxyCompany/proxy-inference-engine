proxy_inference_engine/engine/__init__.py
---
from proxy_inference_engine.engine.inference_engine import InferenceEngine

__all__ = ["InferenceEngine"]


---
proxy_inference_engine/engine/inference_engine.py
---
import logging
from collections.abc import Callable, Iterator

import mlx.core as mx
from pse.structuring_engine import StructuringEngine

from proxy_inference_engine.cache import PromptCache
from proxy_inference_engine.interaction.interaction import Interaction
from proxy_inference_engine.logits_processors import repetition_penalty_logits_processor
from proxy_inference_engine.models import load
from proxy_inference_engine.samplers import make_sampler
from proxy_inference_engine.tokenizer import Tokenizer

logger = logging.getLogger(__name__)


class InferenceEngine:
    """
    A class for performing inference with a LLM.
    """

    def __init__(self, model_path: str):
        self.model, hf_tokenizer = load(model_path)
        self.tokenizer = Tokenizer(hf_tokenizer)
        self.prompt_cache = PromptCache()
        self.structuring_engine = StructuringEngine(
            hf_tokenizer, multi_token_sampling=True
        )
        logger.info(f"Inference Engine initialized with model from {model_path}")

    async def __call__(
        self,
        prompt: list[Interaction],
        **inference_kwargs,
    ) -> tuple[mx.array, str]:
        """
        Generate a completion for the given prompt.

        Args:
            prompt (list[Interaction]): The input prompt for completion.
            **inference_kwargs: Additional keyword arguments to use for inference.
        """
        tokenizer_config = {
            "prompt": prompt,
            **inference_kwargs,
            **self.tokenizer.control_tokens.model_dump(),
        }
        encoded_prompt = self.tokenizer.encode(**tokenizer_config)
        self.prompt_cache.load_cached_prompt(encoded_prompt)
        logger.info(f"PROMPT:\n{self.tokenizer.decode(encoded_prompt)}")
        return await self.generate(encoded_prompt)

    async def generate(
        self,
        prompt_ids: mx.array,
        **inference_kwargs,
    ) -> tuple[mx.array, str]:
        """
        Generate a completion for the given prompt.

        Args:
            prompt_token_ids (mx.array): The input prompt for completion.
        """
        sampler = self.make_sampler(**inference_kwargs)
        logits_processors = self.make_logits_processors(**inference_kwargs)
        max_new_tokens = int(inference_kwargs.get("max_new_tokens", -1))

        result: list[int] = []
        stop_reason: str = "finish"

        for token_id, _ in self.generate_step(
            prompt_ids,
            sampler=sampler,
            logits_processors=logits_processors,
            **inference_kwargs,
        ):
            tokens = token_id.tolist()
            assert isinstance(tokens, list)
            for token_id in tokens:
                if token_id in self.tokenizer.stop_tokens:
                    stop_reason = "stop"
                    break

                result.append(token_id)

            if self.structuring_engine.has_reached_accept_state:
                break

            if len(result) >= max_new_tokens:
                stop_reason = "length"
                break

        return mx.array(result), stop_reason

    def generate_step(
        self,
        prompt_ids: mx.array,
        pixel_values: mx.array | None = None,
        mask: mx.array | None = None,
        sampler: Callable[[mx.array], mx.array] = (lambda x: mx.argmax(x, axis=-1)),
        logits_processors: list[Callable[[mx.array, mx.array], mx.array]] | None = None,
    ) -> Iterator[tuple[mx.array, mx.array]]:
        """
        Generates tokens autoregressively, yielding one token and its log probabilities per step.

        Yields:
            tuples of (next_token_id, log_probabilities).
        """

        def _inference(
            current_input_ids: mx.array,
        ) -> tuple[mx.array, mx.array]:
            """Performs one forward pass, updates history, applies processors, and samples."""
            # Perform the forward pass through the model
            logits = self.model(
                current_input_ids[None],  # Add batch dimension for the model
                pixel_values=pixel_values,
                mask=mask,
                cache=self.prompt_cache.cache,  # Use the KV cache
            )
            # Extract logits for the most recent token
            last_token_logits = logits[:, -1, :]
            self.prompt_cache.update(current_input_ids)

            processed_logits = last_token_logits

            # Apply any configured logits processors sequentially
            current_token_history = self.prompt_cache.computed_ids
            for processor in logits_processors or []:
                processed_logits = processor(current_token_history, processed_logits)

            # Calculate log probabilities (log-softmax normalization)
            logprobs = processed_logits - mx.logsumexp(
                processed_logits, axis=-1, keepdims=True
            )
            # Sample the next token ID using the provided sampler function
            next_token_id = sampler(logprobs)
            return next_token_id, logprobs.squeeze(0)

        # Get the tokens that need to be processed
        tokens_to_process = self.prompt_cache(prompt_ids)
        # Perform the first inference step
        next_token_id, current_logprobs = _inference(tokens_to_process)
        mx.async_eval(next_token_id, current_logprobs)

        step_count = 0
        while True:
            if step_count == 0:
                # Synchronize computation for the first token
                mx.eval(next_token_id)
            else:
                # Perform the next inference step
                next_token_id, current_logprobs = _inference(next_token_id)
                mx.async_eval(next_token_id, current_logprobs)

            # Yield the token and its log probabilities.
            yield next_token_id, current_logprobs

            step_count += 1
            # Periodically clear the MLX computation graph cache to prevent excessive memory growth.
            if step_count % 256 == 0:
                mx.clear_cache()

    def make_sampler(self, **kwargs) -> Callable[[mx.array], mx.array]:
        """
        Return a sampler function.
        If structured is True, use the structured sampler.
        Otherwise, use the simple sampler.
        """
        temp = float(kwargs.get("temp", 1.0))
        min_p = float(kwargs.get("min_p", 0.02))
        min_tokens_to_keep = int(kwargs.get("min_tokens_to_keep", 1))
        top_p = float(kwargs.get("top_p", 1.0))
        top_k = int(kwargs.get("top_k", -1))
        sampler = make_sampler(
            temp=temp,
            min_p=min_p,
            min_tokens_to_keep=min_tokens_to_keep,
            top_p=top_p,
            top_k=top_k,
        )
        if kwargs.get("structured", False) or kwargs.get("json_schema", None):
            return lambda x: self.structuring_engine.sample(x, sampler)
        else:
            return sampler

    def make_logits_processors(self, **kwargs) -> list[Callable[[mx.array, mx.array], mx.array]]:
        """
        Return a list of logits processor functions.
        """
        logits_processors = []
        if kwargs.get("structured", False) or kwargs.get("json_schema", None):
            logits_processors.append(self.structuring_engine.process_logits)

        if kwargs.get("repetition_penalty", 1.0) != 1.0:
            repetition_penalty = float(kwargs.get("repetition_penalty", 1.0))
            context_size = int(kwargs.get("context_size", 60))
            logits_processors.append(repetition_penalty_logits_processor(repetition_penalty, context_size))

        return logits_processors


---
proxy_inference_engine/interaction/__init__.py
---
from enum import Enum


class Role(Enum):
    """
    Enumeration of possible roles for an interaction.
    """

    AGENT = "agent"
    SYSTEM = "system"
    TOOL = "tool"
    USER = "user"

class Type(Enum):
    """
    Enumeration of possible types for an interaction.
    """

    TEXT = "text"
    IMAGE = "image"
    FILE = "file"
    AUDIO = "audio"
    VIDEO = "video"
    ACTION = "action"

from proxy_inference_engine.interaction.content import Content  # noqa: E402
from proxy_inference_engine.interaction.interaction import Interaction  # noqa: E402

__all__ = ["Content", "Interaction"]


---
proxy_inference_engine/interaction/content.py
---
from __future__ import annotations

from typing import Any

from proxy_inference_engine.interaction import Type


class Content:
    """
    Represents the content of an interaction.
    """

    def __init__(self, type: Type, content: Any):
        self.type = type
        self.content = content

    def to_dict(self) -> dict:
        """
        Convert the content to a dictionary representation.
        """
        content_dict = {
            "type": self.type,
        }
        match self.type:
            case Type.TEXT:
                content_dict["text"] = self.content
            case Type.IMAGE:
                content_dict["image_url"] = self.content
            case Type.ACTION:
                content_dict["action"] = self.content
            case _:
                content_dict["file_url"] = self.content

        return content_dict

    @staticmethod
    def text(content: str) -> Content:
        return Content(Type.TEXT, content)


---
proxy_inference_engine/interaction/interaction.py
---
from __future__ import annotations

import json
import uuid
from datetime import datetime
from typing import Any

from proxy_inference_engine.interaction import Role
from proxy_inference_engine.interaction.content import Content


class Interaction:
    """
    Represents a single interaction.

    Interactions are uniquely identified by an event_id and can be converted to
    dictionaries for serialization.
    """

    def __init__(
        self,
        role: Role,
        content: list[Content],
        **kwargs,
    ) -> None:
        """
        Initialize a new Interaction.

        Args:
            event_id: Unique identifier for this interaction (auto-generated if None)
            name: Optional name or identifier for the creator of this interaction
            role: The role of this interaction (SYSTEM, USER, ASSISTANT, or TOOL)
            content: The primary content of the interaction (text, structured data, etc.)
            **kwargs: Additional metadata attributes to store with this interaction
                      Common metadata includes title, color, emoji for display styling
        """
        self.created_at = datetime.now()
        self.event_id = str(uuid.uuid4())
        self.role = role
        self.content = content
        self.metadata = kwargs

    def to_dict(self) -> dict:
        """
        Convert this interaction to a dictionary representation.

        This method serializes the interaction into a dictionary format
        suitable for:
        - Passing to language models as context
        - Storing in memory/databases
        - Converting to JSON for APIs

        Returns:
            A dictionary containing all relevant interaction data
        """
        # Initialize with core attributes
        dict = {
            "event_id": self.event_id,
            "role": self.role.value,
            "content": [content.to_dict() for content in self.content],
        }

        for key, value in self.metadata.items():
            if value and hasattr(value, "to_dict"):
                dict[key] = value.to_dict()
            else:
                dict[key] = value

        return dict

    def __str__(self) -> str:
        """Convert to a JSON string representation for debugging and logging."""
        return json.dumps(self.to_dict(), indent=2)

    def __repr__(self) -> str:
        """Return string representation for REPL and debugging."""
        return self.__str__()

    def __eq__(self, other):
        """
        Check equality by comparing event_ids.

        Two interactions are considered equal if they have the same event_id,
        regardless of any other differences in their content or metadata.
        """
        if not isinstance(other, Interaction):
            return False
        return self.event_id == other.event_id

    def __hash__(self):
        """Create a hash based on event_id for use in sets and dictionaries."""
        return hash(self.event_id)

    def __getattribute__(self, name: str) -> Any:
        """
        Enhanced attribute access that transparently exposes metadata attributes.

        This magic method allows metadata attributes to be accessed directly as if they
        were instance attributes. For example, if an Interaction has metadata["title"],
        you can access it using interaction.title.

        The lookup order is:
        1. Look for actual attributes on the instance
        2. If not found, check if it exists in metadata
        3. If not in metadata, return None

        This creates a more convenient API for accessing metadata fields.
        """
        try:
            # First try to get the actual attribute
            return object.__getattribute__(self, name)
        except AttributeError:
            # If not found, check if it's in metadata
            metadata = object.__getattribute__(self, "metadata")
            if name in metadata:
                return metadata[name]
            return None

    @staticmethod
    def simple(role: Role, content: str) -> Interaction:
        return Interaction(
            role,
            [Content.text(content)],
        )


---
