import secrets
import time
from enum import Enum
from typing import Literal

from pydantic import BaseModel, Field

from proxy_inference_engine.server.models.responses.tools import (
    Function,
    ToolChoice,
    ToolUseMode,
)

# --- Constants ---
RESPONSE_ID_PREFIX = "resp_"
RESPONSE_OBJECT = "response"
MESSAGE_ID_PREFIX = "msg_"
MESSAGE_OBJECT = "message"
OUTPUT_TEXT_OBJECT = "output_text"


def generate_message_id(prefix: str = MESSAGE_ID_PREFIX) -> str:
    random_part = secrets.token_urlsafe(22)
    return f"{prefix}{random_part}"

def generate_response_id(prefix: str = RESPONSE_ID_PREFIX) -> str:
    random_part = secrets.token_urlsafe(22)  # Adjust length as needed
    return f"{prefix}{random_part}"


def get_current_timestamp() -> int:
    return int(time.time())

class OutputTextContent(BaseModel):
    """Represents the text content of an output message."""

    type: Literal["output_text"] = OUTPUT_TEXT_OBJECT
    text: str = Field(description="The generated text content.")

class OutputStatus(Enum):
    """Represents the status of a message."""

    COMPLETED = "completed"
    INCOMPLETE = "incomplete"
    IN_PROGRESS = "in_progress"
    FAILED = "failed"

class OutputMessage(BaseModel):
    """Represents the message output within a response."""

    type: Literal["message"] = "message"
    role: Literal["assistant"] = "assistant"
    id: str = Field(
        default_factory=generate_message_id, description="Unique message identifier."
    )
    status: OutputStatus = OutputStatus.COMPLETED
    content: list[OutputTextContent] = Field(
        description="Content generated by the assistant."
    )


class ResponseUsage(BaseModel):
    """Provides token usage statistics for the chat completion request."""

    input_tokens: int = Field(
        description="The number of tokens constituting the input prompt(s)."
    )
    output_tokens: int = Field(
        description="The total number of tokens generated across all completion choices."
    )
    total_tokens: int = Field(
        description="The sum of `input_tokens` and `output_tokens`."
    )


class ResponseObject(BaseModel):
    """Defines the response schema for the /v1/responses endpoint."""

    id: str = Field(
        default_factory=generate_response_id,
        description="Unique identifier for the response.",
    )
    object: Literal["response"] = RESPONSE_OBJECT
    created_at: int = Field(
        default_factory=get_current_timestamp,
        description="Unix timestamp of response creation.",
    )
    parallel_tool_calls: bool = Field(
        default=False,
        description="Whether to allow the model to run tool calls in parallel.",
    )
    temperature: float = Field(
        default=1.0,
        description="Sampling temperature.",
    )
    tool_choice: str | ToolChoice = Field(
        default=ToolUseMode.AUTO.value,
        description="How the model should select which tool (or tools) to use when generating a response.",
    )
    tools: list[Function] = Field(
        default=[],
        description="A list of tools that the model can use to generate a response.",
    )
    status: OutputStatus = OutputStatus.COMPLETED
    model: str = Field(description="Model ID used for the response.")
    output: list[OutputMessage] = Field(
        description="List containing the output message(s)."
    )
    usage: ResponseUsage = Field(
        description="Usage statistics for the response request."
    )
