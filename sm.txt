src/proxy_inference_engine/server/__init__.py
---


---
src/proxy_inference_engine/server/__main__.py
---
import uvicorn

from proxy_inference_engine.server.app import logger
from proxy_inference_engine.server.config import load_settings

if __name__ == "__main__":
    settings = load_settings()
    logger.info(f"Starting server on {settings.HOST}:{settings.PORT}")
    uvicorn.run(
        "proxy_inference_engine.server.app:app",
        host=settings.HOST,
        port=settings.PORT,
        log_level="info",
    )


---
src/proxy_inference_engine/server/app.py
---
import logging
import logging.config

from fastapi import FastAPI, Request, status
from fastapi.responses import JSONResponse

from proxy_inference_engine.engine import InferenceEngine
from proxy_inference_engine.server.config import load_settings
from proxy_inference_engine.server.dependencies import get_inference_engine
from proxy_inference_engine.server.exceptions import InferenceError
from proxy_inference_engine.server.routes.chat import chat_router
from proxy_inference_engine.server.routes.completions import completions_router
from proxy_inference_engine.server.routes.responses import responses_router

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Global variable to hold the engine
inference_engine: InferenceEngine | None = None

# --- Application Setup ---
def create_app() -> FastAPI:
    global inference_engine

    logger.info("Starting application setup...")
    settings = load_settings()
    logger.info(f"Settings loaded. Model path: {settings.MODEL_PATH}")

    try:
        logger.info(f"Loading InferenceEngine from: {settings.MODEL_PATH}")
        inference_engine = InferenceEngine(settings.MODEL_PATH)
        logger.info("InferenceEngine loaded successfully.")
    except Exception as e:
        logger.exception(f"FATAL: Failed to load InferenceEngine: {e}", exc_info=True)
        raise RuntimeError(f"Could not initialize InferenceEngine: {e}") from e

    # --- Instantiate Services ---
    logger.info("Instantiating services...")

    # --- Create FastAPI App ---
    logger.info("Creating FastAPI application instance...")
    app = FastAPI(
        title="Proxy Inference Engine",
        description="A server for the Proxy Inference Engine.",
        version="0.1.0",
    )

    def _get_loaded_engine() -> InferenceEngine:
        if inference_engine is None:
            raise RuntimeError("Inference engine not initialized")
        return inference_engine

    app.dependency_overrides[get_inference_engine] = _get_loaded_engine

    @app.exception_handler(InferenceError)
    async def inference_exception_handler(request: Request, exc: InferenceError):
        logger.error(f"Caught InferenceError: {exc}", exc_info=True)
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"detail": f"Inference operation failed: {exc}"},
        )

    # Add other exception handlers if needed (e.g., for validation errors)
    logger.info("Exception handlers registered.")

    # --- Include Routers ---
    app.include_router(completions_router, prefix="/v1", tags=["Completions"])
    app.include_router(chat_router, prefix="/v1", tags=["Chat"])
    app.include_router(responses_router, prefix="/v1", tags=["Responses"])
    logger.info("Routers included.")

    logger.info("Application setup complete.")
    return app


# Expose the app instance for uvicorn or other ASGI servers
app = create_app()


---
src/proxy_inference_engine/server/config.py
---
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    MODEL_PATH: str = "mlx-community/gemma-3-12b-it-qat-4bit"
    HOST: str = "0.0.0.0"
    PORT: int = 8000

    model_config = SettingsConfigDict(env_file=".env", extra="ignore")


_settings: Settings | None = None


def load_settings() -> Settings:
    global _settings
    if _settings is None:
        _settings = Settings()
    return _settings


---
src/proxy_inference_engine/server/dependencies.py
---
from typing import Annotated

from fastapi import Depends

from proxy_inference_engine.engine import InferenceEngine


async def get_inference_engine() -> InferenceEngine:
    raise NotImplementedError("Dependency provider not configured")

InferenceEngineDep = Annotated[InferenceEngine, Depends(get_inference_engine)]


---
src/proxy_inference_engine/server/exceptions.py
---
class InferenceError(Exception):
    """Custom exception for inference-related errors."""

    pass


---
src/proxy_inference_engine/server/models/__init__.py
---
from proxy_inference_engine.server.models.chat import (
    ChatCompletionChoice,
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionUsage,
)
from proxy_inference_engine.server.models.completions import (
    CompletionChoice,
    CompletionRequest,
    CompletionResponse,
    CompletionUsage,
)

__all__ = [
    "ChatCompletionChoice",
    "ChatCompletionRequest",
    "ChatCompletionResponse",
    "ChatCompletionUsage",
    "CompletionChoice",
    "CompletionRequest",
    "CompletionResponse",
    "CompletionUsage",
]


---
src/proxy_inference_engine/server/models/chat.py
---
import secrets
import time
from enum import Enum
from typing import Literal

from pydantic import BaseModel, Field

from proxy_inference_engine.interaction import (
    Content,
    Interaction,
    Role,
)

# --- Constants ---
CHAT_COMPLETION_ID_PREFIX = "chatcmpl-"
CHAT_COMPLETION_OBJECT = "chat.completion"


def generate_chat_completion_id(prefix: str = CHAT_COMPLETION_ID_PREFIX) -> str:
    """Generates a unique identifier string for a completion response."""
    random_part = secrets.token_urlsafe(22)
    return f"{prefix}{random_part}"


def get_current_timestamp() -> int:
    """Returns the current time as a Unix epoch timestamp (seconds)."""
    return int(time.time())


class ChatMessage(BaseModel):
    """Represents a single message within the chat conversation."""

    role: str = Field(description="The role of the messages author.")
    content: str = Field(description="The contents of the message.")

    def to_interaction(self) -> Interaction:
        role = Role(self.role)
        return Interaction(
            role,
            [Content.text(self.content)],
        )

class ChatCompletionToolChoice(BaseModel):
    """Defines a tool for the chat completion request."""

    class FunctionName(BaseModel):
        """Defines a function name for the chat completion tool."""

        name: str = Field(description="The name of the function to call.")

    type: Literal["function"] = "function"
    function: FunctionName = Field(description="The function to call.")

    def to_dict(self):
        return {"type": "function", "name": self.function.name}

class ChatCompletionToolUseMode(Enum):
    """Controls which (if any) tool is called by the model."""

    AUTO = "auto"
    REQUIRED = "required"
    NONE = "none"

    def to_dict(self):
        return self.value

class ChatCompletionFunction(BaseModel):
    """Defines a function for the response request."""

    name: str = Field(description="The name of the function to call.")
    type: Literal["function"] = "function"
    description: str = Field(
        description="A description of the function. Used by the model to determine whether or not to call the function."
    )
    strict: bool = Field(
        default=True,
        description="Whether to enforce strict parameter validation.",
    )
    parameters: dict = Field(
        description="A JSON schema object describing the parameters of the function."
    )

class ChatCompletionTool(BaseModel):
    """Defines a tool for the chat completion request."""

    type: Literal["function"] = "function"
    function: ChatCompletionFunction = Field(description="The function to call.")

    def to_dict(self) -> dict:
        return {
            "name": self.function.name,
            "type": "object",
            "description": self.function.description or self.function.name,
            "properties": {
                "name": {"const": self.function.name},
                "arguments": self.function.parameters,
            },
            "strict": self.function.strict,
            "required": ["name", "arguments"],
        }


class ChatCompletionJSONSchemaResponseFormat(BaseModel):
    """Defines the response format for the chat completion request."""

    class JSONSchema(BaseModel):
        """Defines the JSON schema for the response format."""

        name: str = Field(description="The name of the JSON schema.")
        description: str | None = Field(
            default=None,
            description="The description of the JSON schema."
        )
        strict: bool | None = Field(
            default=None,
            description="Whether to enforce strict validation of the JSON schema."
        )
        schema: dict = Field(description="The JSON schema for the response format.")

        model_config = {"protected_namespaces": ()}

    type: Literal["json_schema"] = "json_schema"
    json_schema: JSONSchema = Field(description="The JSON schema for the response format.")

    def to_dict(self):
        return {
            "type": "json_schema",
            **self.json_schema.model_dump()
        }

class ChatCompletionTextResponseFormat(BaseModel):
    """Defines the response format for the chat completion request."""

    type: Literal["text"] = "text"

    def to_dict(self):
        return self.model_dump()

class ChatCompletionRequest(BaseModel):
    """Defines the request schema for the chat completion endpoint."""

    model: str = Field(
        description="The identifier of the model designated for completion generation."
    )
    messages: list[ChatMessage] = Field(
        description="A list of messages comprising the conversation history.",
        min_length=1,
    )
    max_completion_tokens: int | None = Field(
        default=None,
        ge=1,
        description="The upper limit on the number of tokens to generate per completion.",
    )
    temperature: float = Field(
        default=1.0,
        ge=0.0,
        le=2.0,
        description="Controls randomness via sampling temperature.",
    )
    top_p: float | None = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="Implements nucleus sampling.",
    )
    top_k: int | None = Field(
        default=50,
        ge=1,
        le=100,
        description="Controls the number of tokens considered at each step.",
    )
    min_p: float | None = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Minimum probability threshold for token consideration.",
    )
    parallel_tool_calls: bool | None = Field(
        default=None,
        description="Whether to allow the model to run tool calls in parallel.",
    )
    tool_choice: ChatCompletionToolUseMode | ChatCompletionToolChoice | None = Field(
        default=None,
        description="Controls which (if any) tool is called by the model.",
    )
    tools: list[ChatCompletionTool] | None = Field(
        default=None,
        description="A list of tools that the model can use to generate a response.",
    )
    response_format: (
        ChatCompletionTextResponseFormat | ChatCompletionJSONSchemaResponseFormat | None
    ) = Field(
        default=None,
        description="The format of the response.",
    )

class ChatCompletionChoice(BaseModel):
    """Represents a single generated chat completion choice."""

    index: int = Field(description="The index of this choice.")
    message: ChatMessage = Field(description="The message generated by the model.")
    finish_reason: str | None = Field(
        description="Reason generation stopped (e.g., 'stop', 'length', 'tool_calls')."
    )

class ChatCompletionUsage(BaseModel):
    """Provides token usage statistics for the chat completion request."""

    input_tokens: int = Field(
        description="The number of tokens constituting the input prompt(s)."
    )
    output_tokens: int = Field(
        description="The total number of tokens generated across all completion choices."
    )
    total_tokens: int = Field(
        description="The sum of `input_tokens` and `output_tokens`."
    )

# --- Main Response Model ---
class ChatCompletionResponse(BaseModel):
    """Defines the response schema for the chat completion endpoint."""

    id: str = Field(
        default_factory=generate_chat_completion_id,
        description="A unique identifier for the chat completion.",
    )
    object: str = Field(
        default=CHAT_COMPLETION_OBJECT,
        description="The object type, always 'chat.completion'.",
    )
    created: int = Field(
        default_factory=get_current_timestamp,
        description="The Unix timestamp when the completion was created.",
    )
    model: str = Field(description="The model used for the chat completion.")
    choices: list[ChatCompletionChoice] = Field(
        description="A list of chat completion choices."
    )
    usage: ChatCompletionUsage = Field(
        description="Usage statistics for the completion request."
    )
    system_fingerprint: str | None = Field(
        default=None,
        description="System fingerprint representing the backend configuration.",
    )


---
src/proxy_inference_engine/server/models/completions.py
---
import secrets
import time

from pydantic import BaseModel, Field, ValidationInfo, field_validator

# --- Constants ---
COMPLETION_ID_PREFIX = "cmpl-"
TEXT_COMPLETION_OBJECT = "text_completion"

def generate_completion_id(prefix: str = COMPLETION_ID_PREFIX) -> str:
    """Generates a unique identifier for a completion response."""
    random_part = secrets.token_urlsafe(22)
    return f"{prefix}{random_part}"


def get_current_timestamp() -> int:
    """Returns the current time as a Unix epoch timestamp (seconds)."""
    return int(time.time())

class CompletionRequest(BaseModel):
    """
    Defines the request schema for the legacy text completion endpoint (compatible with OpenAI's API).
    """

    model: str = Field(
        description="The identifier of the model designated for completion generation."
    )
    prompt: str | list[str] = Field(
        default="",
        description="The input prompt(s) for which completions are generated. Accepts a single string or a list for batch processing.",
    )
    max_completion_tokens: int = Field(
        default=256,
        ge=1,
        description="The upper limit on the number of tokens to generate per completion.",
    )
    temperature: float = Field(
        default=1.0,
        ge=0.0,
        le=2.0,
        description="Controls randomness via sampling temperature. Values closer to 0.0 yield more deterministic outputs, while higher values increase randomness.",
    )
    top_p: float = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="Implements nucleus sampling by considering only tokens whose cumulative probability mass exceeds this threshold.",
    )
    min_p: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Minimum probability threshold for token consideration during sampling.",
    )
    n: int = Field(
        default=1,
        ge=1,
        description="Specifies the quantity of independent completions to generate for each provided prompt.",
    )
    stream: bool = Field(
        default=False,
        description="Indicates whether to stream intermediate results back as server-sent events.",
    )
    logprobs: int | None = Field(
        default=None,
        ge=0,
        le=5,
        description="If specified, includes the log probabilities for the top `logprobs` most likely tokens at each generation step.",
    )
    best_of: int | None = Field(
        default=None,
        ge=1,
        description="Generates `best_of` completions server-side and returns the one with the highest log probability per token. Requires `best_of > n`.",
    )
    top_k: int = Field(
        default=0,
        ge=0,
        description="Restricts sampling to the `k` most probable tokens. A value of 0 disables top-k filtering.",
    )

    @field_validator("best_of")
    @classmethod
    def check_best_of_greater_than_n(
        cls, v: int | None, info: ValidationInfo
    ) -> int | None:
        """Validates that `best_of` is greater than or equal to `n` if both are provided."""
        # Ensure info.data is accessed safely
        if (
            v is not None
            and "n" in info.data
            and isinstance(info.data.get("n"), int)
            and info.data["n"] > v
        ):
            raise ValueError(
                f"`best_of` ({v}) must be greater than or equal to `n` ({info.data['n']})"
            )
        return v


# --- Response Models ---


class CompletionChoice(BaseModel):
    """Represents a single generated completion choice."""

    index: int = Field(
        description="The sequential index of this choice within the response list."
    )
    text: str = Field(description="The generated completion text for this choice.")
    logprobs: list[float] | None = Field(
        default=None,
        description="Contains log probabilities if requested via the `logprobs` parameter. returns N highest log probabilities for each token.",
    )
    finish_reason: str | None = Field(
        description="The reason the model terminated generation (e.g., 'stop', 'length')."
    )


class CompletionUsage(BaseModel):
    """Provides token usage statistics for the completion request."""

    input_tokens: int = Field(
        description="The number of tokens constituting the input prompt(s)."
    )
    output_tokens: int = Field(
        description="The total number of tokens generated across all completion choices."
    )
    total_tokens: int = Field(
        description="The sum of `input_tokens` and `output_tokens`."
    )


class CompletionResponse(BaseModel):
    """
    Defines the response schema for the legacy text completion endpoint.
    """

    id: str = Field(
        default_factory=generate_completion_id,
        description="A unique identifier assigned to this completion response.",
        examples=["cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7"],
    )
    object: str = Field(
        default=TEXT_COMPLETION_OBJECT,
        description="The type of object returned, consistently 'text_completion'.",
    )
    created: int = Field(
        default_factory=get_current_timestamp,
        description="The Unix timestamp (seconds since epoch) indicating when the response was generated.",
    )
    model: str = Field(
        description="The identifier of the model that executed the completion request."
    )
    choices: list[CompletionChoice] = Field(
        description="A list containing the generated completion(s)."
    )
    usage: CompletionUsage = Field(
        description="An object detailing the token count statistics for the request."
    )
    system_fingerprint: str | None = Field(
        default=None,
        description="An opaque identifier representing the backend configuration that handled the request. May be used for reproducibility tracking.",
    )


---
src/proxy_inference_engine/server/models/responses/__init__.py
---
from proxy_inference_engine.server.models.responses.output import (
    OutputMessage,
    OutputTextContent,
    ResponseObject,
    ResponseUsage,
)
from proxy_inference_engine.server.models.responses.request import ResponseRequest

__all__ = ["OutputMessage", "OutputTextContent", "ResponseObject", "ResponseRequest", "ResponseUsage"]


---
src/proxy_inference_engine/server/models/responses/format.py
---
from __future__ import annotations

from typing import Literal

from pydantic import BaseModel, Field


class ResponseFormat(BaseModel):
    """Defines the response format for the response request."""

    format: TextResponseFormat | JSONSchemaResponseFormat = Field(description="The response format to use.")

    def to_dict(self):
        return self.format.model_dump()

class TextResponseFormat(BaseModel):
    """Defines the response format for the response request."""

    type: Literal["text"] = "text"

class JSONSchemaResponseFormat(BaseModel):
    """Defines the JSON schema for the response format."""

    type: Literal["json_schema"] = "json_schema"
    schema: dict = Field(description="The JSON schema to use.")
    name: str = Field(description="The name of the JSON schema.")
    description: str | None = Field(
        default="",
        description="The description of the JSON schema.",
    )
    strict: bool | None = Field(
        default=False,
        description="Whether to enforce strict validation of the JSON schema.",
    )

    model_config = {
        "protected_namespaces": ()
    }


---
src/proxy_inference_engine/server/models/responses/output.py
---
import secrets
import time
from enum import Enum
from typing import Literal

from pydantic import BaseModel, Field

from proxy_inference_engine.server.models.responses.format import ResponseFormat
from proxy_inference_engine.server.models.responses.tools import (
    Function,
    ToolUseMode,
)

# --- Constants ---
RESPONSE_ID_PREFIX = "resp_"
RESPONSE_OBJECT = "response"
MESSAGE_ID_PREFIX = "msg_"
MESSAGE_OBJECT = "message"
OUTPUT_TEXT_OBJECT = "output_text"
FUNCTION_CALL_ID_PREFIX = "fc_"
TOOL_CALL_ID_PREFIX = "call_"

def generate_message_id(prefix: str = MESSAGE_ID_PREFIX) -> str:
    random_part = secrets.token_urlsafe(22)
    return f"{prefix}{random_part}"

def generate_response_id(prefix: str = RESPONSE_ID_PREFIX) -> str:
    random_part = secrets.token_urlsafe(22)  # Adjust length as needed
    return f"{prefix}{random_part}"

def generate_function_call_id(prefix: str = FUNCTION_CALL_ID_PREFIX) -> str:
    random_part = secrets.token_urlsafe(22)
    return f"{prefix}{random_part}"

def generate_tool_call_id(prefix: str = TOOL_CALL_ID_PREFIX) -> str:
    random_part = secrets.token_urlsafe(22)
    return f"{prefix}{random_part}"

def get_current_timestamp() -> int:
    return int(time.time())

class OutputTextContent(BaseModel):
    """Represents the text content of an output message."""

    type: Literal["output_text"] = OUTPUT_TEXT_OBJECT
    text: str = Field(description="The generated text content.")

class OutputStatus(Enum):
    """Represents the status of a message."""

    COMPLETED = "completed"
    INCOMPLETE = "incomplete"
    IN_PROGRESS = "in_progress"
    FAILED = "failed"

class OutputMessageType(Enum):
    """Represents the type of a message."""

    MESSAGE = "message"
    FUNCTION_CALL = "function_call"
    REASONING = "reasoning"

class OutputMessage(BaseModel):
    """Represents the message output within a response."""

    type: OutputMessageType = Field(default=OutputMessageType.MESSAGE, description="The type of the message.")
    status: OutputStatus = Field(default=OutputStatus.COMPLETED, description="The status of the message.")
    role: Literal["assistant"] = "assistant"
    id: str = Field(
        default_factory=generate_message_id, description="Unique message identifier."
    )
    content: list[OutputTextContent] = Field(
        description="Content generated by the assistant."
    )

class OutputFunctionCall(BaseModel):
    """Represents the function call output within a response."""

    type: Literal["function_call"] = "function_call"
    name: str = Field(description="The name of the function to call.")
    id: str = Field(
        default_factory=generate_function_call_id, description="Unique function call identifier."
    )
    call_id: str = Field(
        default_factory=generate_tool_call_id, description="Unique tool call identifier."
    )
    arguments: str = Field(description="The arguments to pass to the function.")
    status: OutputStatus = Field(default=OutputStatus.COMPLETED, description="The status of the function call.")


class ResponseUsage(BaseModel):
    """Provides token usage statistics for the chat completion request."""

    input_tokens: int = Field(
        description="The number of tokens constituting the input prompt(s)."
    )
    output_tokens: int = Field(
        description="The total number of tokens generated across all completion choices."
    )
    total_tokens: int = Field(
        description="The sum of `input_tokens` and `output_tokens`."
    )


class ResponseObject(BaseModel):
    """Defines the response schema for the /v1/responses endpoint."""

    id: str = Field(
        default_factory=generate_response_id,
        description="Unique identifier for the response.",
    )
    object: Literal["response"] = RESPONSE_OBJECT
    created_at: int = Field(
        default_factory=get_current_timestamp,
        description="Unix timestamp of response creation.",
    )
    parallel_tool_calls: bool = Field(
        default=False,
        description="Whether to allow the model to run tool calls in parallel.",
    )
    temperature: float | None = Field(
        default=None,
        description="Sampling temperature.",
    )
    top_p: float | None = Field(
        default=None,
        description="Nucleus sampling threshold.",
    )
    top_k: int | None = Field(
        default=None,
        description="Controls the number of tokens considered at each step.",
    )
    min_p: float | None = Field(
        default=None,
        description="Minimum probability threshold for token consideration.",
    )
    tool_choice: ToolUseMode = Field(
        default=ToolUseMode.AUTO,
        description="How the model should select which tool (or tools) to use when generating a response.",
    )
    tools: list[Function] = Field(
        default=[],
        description="A list of tools that the model can use to generate a response.",
    )
    status: OutputStatus = OutputStatus.COMPLETED
    model: str = Field(description="Model ID used for the response.")
    output: list[OutputMessage | OutputFunctionCall] = Field(
        description="List containing the output message(s)."
    )
    usage: ResponseUsage = Field(
        description="Usage statistics for the response request."
    )
    text: ResponseFormat | None = Field(
        default=None,
        description="The response format to use.",
    )


---
src/proxy_inference_engine/server/models/responses/request.py
---
from pydantic import BaseModel, Field

from proxy_inference_engine.server.models.responses.format import ResponseFormat
from proxy_inference_engine.server.models.responses.tools import (
    Function,
    FunctionID,
    ToolUseMode,
)


class ResponseRequest(BaseModel):
    """Defines the request schema for the /v1/responses endpoint (MVP)."""

    model: str = Field(description="Model ID used to generate the response.")
    input: str = Field(description="Text input to the model.")
    stream: bool | None = Field(
        default=None,
        description="Whether to stream the response.",
    )
    parallel_tool_calls: bool | None = Field(
        default=None,
        description="Whether to allow the model to run tool calls in parallel.",
    )
    instructions: str | None = Field(
        default=None,
        description="System/developer instructions for the model.",
    )
    max_output_tokens: int | None = Field(
        default=None,
        ge=1,
        description="Upper bound for the number of tokens generated.",
    )
    temperature: float | None = Field(
        default=None,
        ge=0.0,
        le=2.0,
        description="Sampling temperature.",
    )
    top_p: float | None = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Nucleus sampling threshold.",
    )
    top_k: int | None = Field(
        default=None,
        ge=1,
        le=100,
        description="Controls the number of tokens considered at each step.",
    )
    min_p: float | None = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Minimum probability threshold for token consideration.",
    )
    tool_choice: ToolUseMode | FunctionID = Field(
        default=ToolUseMode.AUTO,
        description="How the model should select which tool (or tools) to use when generating a response.",
    )
    tools: list[Function] | None = Field(
        default=None,
        description="A list of tools that the model can use to generate a response.",
    )
    text: ResponseFormat | None = Field(
        default=None,
        description="The format of the response.",
    )


---
src/proxy_inference_engine/server/models/responses/tools.py
---
from enum import Enum
from typing import Literal

from pydantic import BaseModel, Field


class Function(BaseModel):
    """Defines a function for the response request."""

    name: str = Field(description="The name of the function to call.")
    type: Literal["function"] = "function"
    description: str = Field(
        description="A description of the function. Used by the model to determine whether or not to call the function."
    )
    strict: bool = Field(
        default=True,
        description="Whether to enforce strict parameter validation.",
    )
    parameters: dict = Field(
        description="A JSON schema object describing the parameters of the function."
    )

    def to_dict(self) -> dict:
        return {
            "name": self.name,
            "type": "object",
            "description": self.description or self.name,
            "properties": {
                "name": {"const": self.name},
                "arguments": self.parameters,
            },
            "strict": self.strict,
            "required": ["name", "arguments"],
        }

class ToolUseMode(Enum):
    """Controls which (if any) tool is called by the model."""

    AUTO = "auto"
    REQUIRED = "required"
    NONE = "none"

    def to_dict(self):
        return self.value

class FunctionID(BaseModel):
    """Defines a function tool for the response request."""

    type: Literal["function"] = "function"
    name: str = Field(description="The name of the function to call.")

    def to_dict(self):
        return self.model_dump()


---
src/proxy_inference_engine/server/routes/__init__.py
---
 

---
src/proxy_inference_engine/server/routes/chat.py
---
import logging

from fastapi import APIRouter, Depends, HTTPException, status

from proxy_inference_engine.engine.inference_engine import InferenceEngine
from proxy_inference_engine.interaction import Interaction
from proxy_inference_engine.server.app import get_inference_engine
from proxy_inference_engine.server.exceptions import InferenceError
from proxy_inference_engine.server.models.chat import (
    ChatCompletionChoice,
    ChatCompletionRequest,
    ChatCompletionResponse,
    ChatCompletionUsage,
    ChatMessage,
)

logger = logging.getLogger(__name__)

chat_router = APIRouter()


@chat_router.post(
    "/chat/completions",
    response_model=ChatCompletionResponse,
    summary="Create a chat completion",
    tags=["Chat"],
)
async def handle_completion_request(
    request: ChatCompletionRequest,
    engine: InferenceEngine = Depends(get_inference_engine),  # noqa: B008
) -> ChatCompletionResponse:
    """
    Handles requests to the `/v1/chat/completions` endpoint.
    """
    logger.info(f"Handling chat completion request for model: {request.model}")

    input_interactions: list[Interaction] = [
        msg.to_interaction()
        for msg in request.messages
    ]

    inference_kwargs = {
        "max_completion_tokens": request.max_completion_tokens,
        "temperature": request.temperature,
        "top_p": request.top_p,
        "top_k": request.top_k,
        "min_p": request.min_p,
        "parallel_tool_calls": request.parallel_tool_calls,
        "tool_choice": request.tool_choice.to_dict() if request.tool_choice else None,
        "tools": [tool.to_dict() for tool in request.tools] if request.tools else None,
        "response_format": request.response_format.to_dict() if request.response_format else None,
    }
    inference_kwargs = {k: v for k, v in inference_kwargs.items() if v is not None}

    try:
        generated_text, metadata = await engine(
            input_interactions,
            **inference_kwargs,
        )
    except InferenceError as e:
        logger.error(f"Inference error processing request: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Inference failed: {e}",
        ) from e
    except NotImplementedError as e:
        logger.error(f"Feature not implemented: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail=str(e),
        ) from e
    except Exception as e:
        logger.exception("An unexpected error occurred", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred during completion.",
        ) from e

    finish_reason = metadata.get("finish_reason", "unknown")
    prompt_tokens = metadata.get("prompt_tokens", 0)
    completion_tokens = metadata.get("completion_tokens", 0)
    total_tokens = metadata.get("total_tokens", 0)

    choice = ChatCompletionChoice(
        index=0,
        message=ChatMessage(
            role="assistant",
            content=generated_text,
        ),
        finish_reason=finish_reason,
    )

    usage = ChatCompletionUsage(
        input_tokens=prompt_tokens,
        output_tokens=completion_tokens,
        total_tokens=total_tokens,
    )

    response = ChatCompletionResponse(
        model=request.model,
        choices=[choice],
        usage=usage,
    )
    logger.info(f"Chat completion request successful. ID: {response.id}")
    return response


---
src/proxy_inference_engine/server/routes/completions.py
---
import logging

from fastapi import APIRouter, Depends, HTTPException, status

from proxy_inference_engine.engine.inference_engine import InferenceEngine
from proxy_inference_engine.interaction import (
    Interaction,
    Role,
)
from proxy_inference_engine.server.dependencies import get_inference_engine
from proxy_inference_engine.server.exceptions import InferenceError
from proxy_inference_engine.server.models.completions import (
    CompletionChoice,
    CompletionRequest,
    CompletionResponse,
    CompletionUsage,
)

logger = logging.getLogger(__name__)

completions_router = APIRouter()


@completions_router.post(
    "/completions",
    response_model=CompletionResponse,
    summary="Create a text completion",
    tags=["Completions"],
)
async def handle_completion_request(
    request: CompletionRequest,
    engine: InferenceEngine = Depends(get_inference_engine),  # noqa: B008
) -> CompletionResponse:
    """
    Handles requests to the `/completions` endpoint.

    This endpoint uses the OpenAI v1 chat completions API.
    """
    logger.info(f"Handling completion request for model: {request.model}")

    if request.stream:
        logger.warning("Streaming requested but not supported.")
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail="Streaming is not supported in this version.",
        )
    if request.n > 1:
        logger.warning("Request parameter 'n' > 1 is not supported, using n=1.")
        # If best_of is also set, log a warning or raise error based on validation
        if request.best_of is not None and request.best_of > 1:
            logger.warning(
                "Request parameters 'n' > 1 and 'best_of' > 1 are not supported, using n=1 and ignoring best_of."
            )

    input_interactions: list[Interaction]
    if isinstance(request.prompt, str):
        input_interactions = [
            Interaction.simple(
                role=Role.USER,
                content=request.prompt,
            )
        ]
    elif isinstance(request.prompt, list) and len(request.prompt) > 0:
        logger.warning(
            "Batch prompt input received, using only the first prompt in this version."
        )
        input_interactions = [
            Interaction.simple(
                role=Role.USER,
                content=request.prompt[0],
            )
        ]
    else:
        raise HTTPException(
            status_code=400,
            detail="Invalid prompt format. Expecting string or list of strings.",
        )

    inference_kwargs = {
        "max_completion_tokens": request.max_completion_tokens,
        "temperature": request.temperature,
        "top_p": request.top_p,
        "top_k": request.top_k,
        "min_p": request.min_p
    }
    
    try:
        generated_text, metadata = await engine(
            input_interactions,
            **inference_kwargs,
        )
    except InferenceError as e:
        logger.error(f"Inference error processing request: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Inference failed: {e}",
        ) from e
    except NotImplementedError as e:
        logger.error(f"Feature not implemented: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail=str(e),
        ) from e
    except Exception as e:
        logger.exception("An unexpected error occurred", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred during completion.",
        ) from e

    finish_reason = metadata.get("finish_reason", "unknown")
    prompt_tokens = metadata.get("prompt_tokens", 0)
    completion_tokens = metadata.get("completion_tokens", 0)
    total_tokens = metadata.get("total_tokens", 0)

    choice = CompletionChoice(
        index=0,
        text=generated_text,
        logprobs=None,
        finish_reason=finish_reason,
    )

    usage = CompletionUsage(
        input_tokens=prompt_tokens,
        output_tokens=completion_tokens,
        total_tokens=total_tokens,
    )

    response = CompletionResponse(
        model=request.model,
        choices=[choice],
        usage=usage,
    )
    logger.info(f"Completion request successful. ID: {response.id}")
    return response


---
src/proxy_inference_engine/server/routes/responses.py
---
import logging

from fastapi import APIRouter, Depends, HTTPException, status

from proxy_inference_engine.engine import InferenceEngine
from proxy_inference_engine.interaction import Interaction, Role
from proxy_inference_engine.server.dependencies import get_inference_engine
from proxy_inference_engine.server.exceptions import InferenceError
from proxy_inference_engine.server.models.responses import (
    OutputMessage,
    OutputTextContent,
    ResponseObject,
    ResponseRequest,
    ResponseUsage,
)

logger = logging.getLogger(__name__)

responses_router = APIRouter()


@responses_router.post(
    "/responses",
    response_model=ResponseObject,
    summary="Create a model response",
    tags=["Responses"],
)
async def handle_response_request(
    request: ResponseRequest,
    engine: InferenceEngine = Depends(get_inference_engine),  # noqa: B008
) -> ResponseObject:
    """
    Handles requests to the `/v1/responses` endpoint (MVP: text-only).
    """
    logger.info(f"Handling response request for model: {request.model}")

    input_interactions: list[Interaction] = []
    if request.instructions:
        input_interactions.append(
            Interaction.simple(role=Role.SYSTEM, content=request.instructions)
        )
    input_interactions.append(
        Interaction.simple(role=Role.USER, content=request.input)
    )

    inference_kwargs = {
        "max_completion_tokens": request.max_output_tokens,
        "temperature": request.temperature,
        "top_p": request.top_p,
        "top_k": request.top_k,
        "min_p": request.min_p,
        "parallel_tool_calls": request.parallel_tool_calls,
        "tool_choice": request.tool_choice.to_dict() if request.tool_choice else None,
        "tools": [tool.to_dict() for tool in request.tools] if request.tools else None,
        "response_format": request.text.to_dict() if request.text else None,
    }
    inference_kwargs = {k: v for k, v in inference_kwargs.items() if v is not None}

    try:
        generated_text, metadata = await engine(
            input_interactions,
            **inference_kwargs,
        )
    except InferenceError as e:
        logger.error(f"Inference error processing request: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Inference failed: {e}",
        ) from e
    except NotImplementedError as e:
        logger.error(f"Feature not implemented: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail=str(e),
        ) from e
    except Exception as e:
        logger.exception("An unexpected error occurred", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred during completion.",
        ) from e

    prompt_tokens = metadata.get("prompt_tokens", 0)
    completion_tokens = metadata.get("completion_tokens", 0)
    total_tokens = metadata.get("total_tokens", 0)

    output_content = OutputTextContent(text=generated_text)
    response_message = OutputMessage(content=[output_content])
    usage = ResponseUsage(
        input_tokens=prompt_tokens,
        output_tokens=completion_tokens,
        total_tokens=total_tokens,
    )

    response = ResponseObject(
        model=request.model,
        output=[response_message],
        usage=usage,
    )
    logger.info(f"Response request successful. ID: {response.id}")
    return response


---
src/proxy_inference_engine/state_machine/__init__.py
---
from proxy_inference_engine.state_machine.root import RootStateMachine

__all__ = ["RootStateMachine"]


---
src/proxy_inference_engine/state_machine/root.py
---
from typing import Any

from pse.structuring_engine import StateMachine
from pse.types.base.any import AnyStateMachine
from pse_core import StateGraph

from proxy_inference_engine.state_machine.sub_state import SubState
from proxy_inference_engine.state_machine.sub_states import (
    FreeformTextState,
    StructuredOutputState,
    ToolCallState,
)


class RootStateMachine(StateMachine):
    """The root state machine for the proxy inference engine."""

    def __init__(self):
        """
        Initialize the root state machine.
        """
        self.available_states: dict[str, SubState] = {}
        super().__init__(
            self._create_state_graph(),
            start_state="start",
            end_states=["end"],
            identifier="root",
        )

    def configure(
        self,
        response_format: dict[str, Any],
        tools: list[dict[str, Any]] | None = None,
        tool_delimiters: tuple[str, str] | None = None,
        parallel_tool_calls: bool | None = None,
        tool_choice: str | dict[str, Any] | None = None,
    ):
        """
        Configure the root state machine.
        """
        self.state_graph = self._create_state_graph(
            response_format,
            tools,
            tool_delimiters,
            parallel_tool_calls,
            tool_choice,
        )


    def _create_state_graph(
        self,
        response_format: dict[str, Any] | None = None,
        tools: list[dict[str, Any]] | None = None,
        tool_delimiters: tuple[str, str] | None = None,
        parallel_tool_calls: bool | None = None,
        tool_choice: str | dict[str, Any] | None = None,
    ) -> StateGraph:
        """
        Create and configure all states for the root state machine.
        """

        response_format = response_format or {"type": "text"}

        if response_format.get("type") == "json_schema":
            structured_output_state = StructuredOutputState(response_format, delimiters=None)
            self.available_states[structured_output_state.identifier] = structured_output_state
        elif tools and tool_choice != "none":
            tool_call_state = ToolCallState(
                tools,
                tool_delimiters,
                tool_choice,
                parallel_tool_calls,
            )
            self.available_states[tool_call_state.identifier] = tool_call_state

        if response_format.get("type") == "text" and tool_choice != "required":
            freeform_text = FreeformTextState()
            self.available_states[freeform_text.identifier] = freeform_text

        states = [state.state_machine for state in self.available_states.values()]
        root_state_machine: StateMachine = (
            AnyStateMachine(states)
            if len(states) > 1
            else states[0]
        )

        return { "start": [(root_state_machine, "end")] }


---
src/proxy_inference_engine/state_machine/sub_state.py
---
from abc import abstractmethod

from pse_core.state_machine import StateMachine


class SubState:
    """
    A sub state for the root state machine.
    """

    def __init__(self, identifier: str):
        self.identifier = identifier

    @property
    @abstractmethod
    def state_machine(self) -> StateMachine:
        pass


---
src/proxy_inference_engine/state_machine/sub_states/__init__.py
---
from proxy_inference_engine.state_machine.sub_states.freeform_text import (
    FreeformTextState,
)
from proxy_inference_engine.state_machine.sub_states.reasoning import ReasoningState
from proxy_inference_engine.state_machine.sub_states.structured_output import (
    StructuredOutputState,
)
from proxy_inference_engine.state_machine.sub_states.tool_call import ToolCallState

__all__ = [
    "FreeformTextState",
    "ReasoningState",
    "StructuredOutputState",
    "ToolCallState",
]


---
src/proxy_inference_engine/state_machine/sub_states/freeform_text.py
---
from pse.types.base.character import CharacterStateMachine
from pse_core import StateGraph
from pse_core.state_machine import StateMachine

from proxy_inference_engine.state_machine.sub_state import SubState


class FreeformTextState(SubState):
    """
    State for freeform text.
    """

    def __init__(self, secondary_state_machine: StateMachine | None = None):
        """
        Initialize a new FreeformTextState.

        Args:
            delimiters: delimiters for the freeform text state
        """
        super().__init__(identifier="text_output")
        self.secondary_state_machine = secondary_state_machine

    @property
    def state_machine(self) -> StateMachine:
        """
        Create a freeform state machine for reasoning.

        Returns:
            A StateMachine instance configured for freeform reasoning
        """
        return CharacterStateMachine()

class FreeformTextStateMachine(StateMachine):
    """
    State for freeform text.
    """

    def __init__(self, secondary_state_machine: StateMachine | None = None):
        text_output_sm = CharacterStateMachine()
        text_output_sm.identifier = "text_output"
        state_graph: StateGraph = {
            "start": [(text_output_sm, "end")],
        }
        if secondary_state_machine:
            state_graph["end"] = [(secondary_state_machine, "super_end")]

        super().__init__(
            state_graph,
            start_state="start",
            end_states=["end", "super_end"],
            identifier="freeform_text_with_wait_for",
        )


---
src/proxy_inference_engine/state_machine/sub_states/reasoning.py
---
from pse.types.misc.fenced_freeform import FencedFreeformStateMachine
from pse_core.state_machine import StateMachine

from proxy_inference_engine.state_machine.sub_state import SubState


class ReasoningState(SubState):
    """
    State for freeform reasoning.
    """

    def __init__(
        self,
        delimiters: tuple[str, str] | None = None,
    ):
        """
        Initialize a new StructuredOutputState.

        Args:
            delimiters: delimiters for the freeform reasoning state
        """
        super().__init__(identifier="reasoning")
        self.delimiters = delimiters or ("```thinking\n", "\n```")

    @property
    def state_machine(self) -> StateMachine:
        """
        Create a freeform state machine for reasoning.

        Returns:
            A StateMachine instance configured for freeform reasoning
        """
        return FencedFreeformStateMachine(
            self.identifier,
            self.delimiters,
            char_min=50,
        )


---
src/proxy_inference_engine/state_machine/sub_states/structured_output.py
---
from typing import Any

from pse.types.json import json_schema_state_machine
from pse_core.state_machine import StateMachine

from proxy_inference_engine.state_machine.sub_state import SubState


class StructuredOutputState(SubState):
    """
    State for handling structured output during agent execution.
    """

    def __init__(
        self,
        json_schema: dict[str, Any],
        delimiters: tuple[str, str] | None = ("```json\n", "\n```"),
    ):
        """
        Initialize a new StructuredOutputState.

        Args:
            json_schema: JSON schema for the structured output
            delimiters: Optional custom delimiters for the structured output state
        """
        super().__init__(identifier="structured_output")
        self.delimiters = delimiters
        self.json_schema = json_schema

    @property
    def state_machine(self) -> StateMachine:
        """
        Create a JSON schema-based state machine for structured output.

        This property dynamically generates a state machine that validates structured
        output against their JSON schemas, ensuring all required parameters are provided
        and correctly formatted.

        Returns:
            A StateMachine instance configured for structured output validation
        """
        _, state_machine = json_schema_state_machine(
            self.json_schema,
            self.delimiters
        )
        state_machine.identifier = self.identifier
        return state_machine


---
src/proxy_inference_engine/state_machine/sub_states/tool_call.py
---
from typing import Any

from pse.types.json import json_schema_state_machine
from pse_core.state_machine import StateMachine

from proxy_inference_engine.state_machine.sub_state import SubState


class ToolCallState(SubState):
    """
    State for handling tool calls during agent execution.

    The ToolCallState enables agents to interact with their environment by providing
    a structured interface for invoking external tools. It creates a JSON schema-based
    state machine that ensures tool calls have the correct format, required parameters,
    and follow the expected structure.

    This state is a key component of the agent's ability to take action in the world,
    allowing it to perform operations like web searches, calculations, or API calls.
    """

    def __init__(
        self,
        tools: list[dict[str, Any]],
        delimiters: tuple[str, str] | None = ("```tool\n", "\n```"),
        tool_choice: str | dict[str, Any] | None = None,
        parallel_tool_calls: bool | None = None,
    ):
        """
        Initialize a new ToolCallState.

        Args:
            tools: List of Tool objects available for use in this state
            delimiters: Optional custom delimiters for the tool call state
            list_delimiters: Optional delimiters for the tool list in the prompt
        """
        super().__init__(identifier="tool_call")
        self.delimiters = delimiters
        self.tool_choice = tool_choice
        self.parallel_tool_calls = parallel_tool_calls

        if tool_choice is None or (isinstance(tool_choice, str) and tool_choice != "none"):
            self.tools = tools
        elif isinstance(tool_choice, dict) and "name" in tool_choice:
            requested_function_name = tool_choice["name"]
            self.tools = [tool for tool in tools if tool["name"] == requested_function_name]
        else:
            self.tools = []

    @property
    def state_machine(self) -> StateMachine:
        """
        Create a JSON schema-based state machine for tool calls.

        This property dynamically generates a state machine that validates tool calls
        against their JSON schemas, ensuring all required parameters are provided and
        correctly formatted.

        Returns:
            A StateMachine instance configured for tool invocation validation
        """
        if self.parallel_tool_calls:
            schema = {"type": "array", "items": {"oneOf": self.tools}}
        else:
            schema = self.tools

        _, state_machine = json_schema_state_machine(schema, self.delimiters)
        state_machine.identifier = self.identifier
        return state_machine


---
src/proxy_inference_engine/engine/__init__.py
---
from proxy_inference_engine.engine.inference_engine import InferenceEngine

__all__ = ["InferenceEngine"]


---
src/proxy_inference_engine/engine/inference_engine.py
---
import logging
from collections.abc import Callable, Iterator
from typing import Any

import mlx.core as mx
from pse.structuring_engine import StructuringEngine

from proxy_inference_engine.cache import PromptCache
from proxy_inference_engine.interaction.interaction import Interaction
from proxy_inference_engine.logits_processors import repetition_penalty_logits_processor
from proxy_inference_engine.models import load
from proxy_inference_engine.samplers import make_sampler
from proxy_inference_engine.state_machine import RootStateMachine
from proxy_inference_engine.tokenizer import Tokenizer

logger = logging.getLogger(__name__)


class InferenceEngine:
    """
    A class for performing inference with a LLM.
    """

    def __init__(self, model_path: str):
        llm = load(model_path)
        self.model, self.tokenizer_config = llm.model, llm.tokenizer_config
        self.tokenizer = Tokenizer(llm.hf_tokenizer, self.tokenizer_config)
        self.prompt_cache = PromptCache()
        self.structuring_engine = StructuringEngine(
            llm.hf_tokenizer,
            whitelist_control_tokens=self.tokenizer.control_tokens.end_tokens(),
            multi_token_sampling=True
        )
        self.root_state_machine = RootStateMachine()
        logger.info(f"Inference Engine initialized with model from {model_path}")

    async def __call__(
        self,
        prompt: list[Interaction],
        **inference_kwargs,
    ) -> tuple[str, dict[str, Any]]:
        """
        Generate a completion for the given prompt.

        Args:
            prompt (list[Interaction]): The input prompt for completion.
            **inference_kwargs: Additional keyword arguments to use for inference.
        """
        tokenizer_config = {
            "prompt": prompt,
            **inference_kwargs,
            **self.tokenizer.control_tokens.model_dump(),
        }
        encoded_prompt = self.tokenizer.encode(**tokenizer_config)
        prompt_length = encoded_prompt.size

        self.prompt_cache.load_cached_prompt(encoded_prompt)
        logger.info(f"PROMPT:\n{self.tokenizer.decode(encoded_prompt)}")

        self.root_state_machine.configure(**inference_kwargs)
        self.structuring_engine.reset()
        self.structuring_engine.configure(self.root_state_machine)

        generated_ids, finish_reason = await self.generate(encoded_prompt, **inference_kwargs)
        logger.info(f"\nGENERATED:\n{self.tokenizer.decode(generated_ids)}\n\n")

        for state_id, output in self.structuring_engine.get_stateful_structured_output():
            engine_state = self.root_state_machine.available_states.get(state_id)
            if not engine_state:
                logger.warning(f"Unknown state: {state_id}")
                continue

            logger.info(f"STATE: {engine_state.identifier}")
            logger.info(f"OUTPUT: {output}")

            match engine_state.identifier:
                case "structured_output":
                    pass
                case "tool_call":
                    pass
                case "freeform_text":
                    pass
                case _:
                    logger.warning(f"Unknown state: {engine_state.identifier}")

        return self.tokenizer.decode(generated_ids), {
            "finish_reason": finish_reason,
            "prompt_tokens": prompt_length,
            "completion_tokens": len(generated_ids),
            "total_tokens": prompt_length + len(generated_ids),
        }

    async def generate(
        self,
        prompt_ids: mx.array,
        **inference_kwargs,
    ) -> tuple[list[int], str]:
        """
        Generate a completion for the given prompt.

        Args:
            prompt_token_ids (mx.array): The input prompt for completion.
        """
        sampler = self.make_sampler(**inference_kwargs)
        logits_processors = self.make_logits_processors(**inference_kwargs)
        max_completion_tokens = int(inference_kwargs.get("max_completion_tokens", -1))

        result: list[int] = []
        stop_reason: str = "finish"

        for token_id, _ in self.generate_step(
            prompt_ids,
            sampler=sampler,
            logits_processors=logits_processors,
        ):
            tokens = token_id.tolist()
            assert isinstance(tokens, list)
            for token_id in tokens:
                if token_id in self.tokenizer.stop_tokens:
                    stop_reason = "stop"
                    break

                result.append(token_id)

            if self.structuring_engine.has_reached_accept_state:
                stop_reason = "tool_calls"
                break

            if max_completion_tokens > 0 and len(result) >= max_completion_tokens:
                stop_reason = "length"
                break

        return result, stop_reason

    def generate_step(
        self,
        prompt_ids: mx.array,
        pixel_values: mx.array | None = None,
        mask: mx.array | None = None,
        sampler: Callable[[mx.array], mx.array] = (lambda x: mx.argmax(x, axis=-1)),
        logits_processors: list[Callable[[mx.array, mx.array], mx.array]] | None = None,
    ) -> Iterator[tuple[mx.array, mx.array]]:
        """
        Generates tokens autoregressively, yielding one token and its log probabilities per step.

        Yields:
            tuples of (next_token_id, log_probabilities).
        """

        def _inference(current_input_ids: mx.array) -> tuple[mx.array, mx.array]:
            """Performs one forward pass, updates history, applies processors, and samples."""
            model_kwargs: dict[str, Any] = {"cache": self.prompt_cache.cache}

            # Only add optional parameters if they exist
            if pixel_values is not None:
                model_kwargs["pixel_values"] = pixel_values
            if mask is not None:
                model_kwargs["mask"] = mask

            # Call model with appropriate arguments
            logits = self.model(current_input_ids[None], **model_kwargs)
            # Extract logits for the most recent token
            last_token_logits = logits[:, -1, :]
            self.prompt_cache.update(current_input_ids)

            processed_logits = last_token_logits

            # Apply any configured logits processors sequentially
            current_token_history = self.prompt_cache.computed_ids
            for processor in logits_processors or []:
                processed_logits = processor(current_token_history, processed_logits)

            # Calculate log probabilities (log-softmax normalization)
            logprobs = processed_logits - mx.logsumexp(
                processed_logits, axis=-1, keepdims=True
            )
            # Sample the next token ID using the provided sampler function
            next_token_id = sampler(logprobs)
            return next_token_id, logprobs.squeeze(0)

        if len(self.prompt_cache.cache) == 0:
            self.prompt_cache.create_kv_cache(self.model)

        tokens_to_process = self.prompt_cache(prompt_ids)
        next_token_id, current_logprobs = _inference(tokens_to_process)
        mx.async_eval(next_token_id, current_logprobs)

        step_count = 0
        while True:
            if step_count == 0:
                # Synchronize computation for the first token
                mx.eval(next_token_id)
            else:
                # Perform the next inference step
                next_token_id, current_logprobs = _inference(next_token_id)
                mx.async_eval(next_token_id, current_logprobs)

            # Yield the token and its log probabilities.
            yield next_token_id, current_logprobs

            step_count += 1
            # Periodically clear the MLX computation graph cache to prevent excessive memory growth.
            if step_count % 256 == 0:
                mx.clear_cache()

    def make_sampler(self, **kwargs) -> Callable[[mx.array], mx.array]:
        """
        Return a sampler function.
        If structured is True, use the structured sampler.
        Otherwise, use the simple sampler.
        """
        temp = float(kwargs.get("temp", 1.0))
        min_p = float(kwargs.get("min_p", 0.02))
        min_tokens_to_keep = int(kwargs.get("min_tokens_to_keep", 1))
        top_p = float(kwargs.get("top_p", 1.0))
        top_k = int(kwargs.get("top_k", -1))
        sampler = make_sampler(
            temp=temp,
            min_p=min_p,
            min_tokens_to_keep=min_tokens_to_keep,
            top_p=top_p,
            top_k=top_k,
        )
        if kwargs.get("structured", False) or kwargs.get("json_schema", None):
            return lambda x: self.structuring_engine.sample(x, sampler)
        else:
            return sampler

    def make_logits_processors(
        self, **kwargs
    ) -> list[Callable[[mx.array, mx.array], mx.array]]:
        """
        Return a list of logits processor functions.
        """
        logits_processors = []
        if kwargs.get("structured", False) or kwargs.get("json_schema", None):
            logits_processors.append(self.structuring_engine.process_logits)

        if kwargs.get("repetition_penalty", 1.0) != 1.0:
            repetition_penalty = float(kwargs.get("repetition_penalty", 1.0))
            context_size = int(kwargs.get("context_size", 60))
            logits_processors.append(
                repetition_penalty_logits_processor(repetition_penalty, context_size)
            )

        return logits_processors


---
